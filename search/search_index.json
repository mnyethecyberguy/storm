{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Blog","text":""},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2023/07/19/aws-virtual-networking-default-settings/","title":"AWS Virtual Networking Default Settings","text":""},{"location":"blog/2023/07/19/aws-virtual-networking-default-settings/#default-vpcs","title":"Default VPCs","text":""},{"location":"blog/2023/07/19/aws-virtual-networking-default-settings/#default-security-groups","title":"Default Security Groups","text":""},{"location":"blog/2023/07/19/aws-virtual-networking-default-settings/#deleting-the-default-vpc-and-default-subnets","title":"Deleting the Default VPC and Default Subnets","text":"<p>This bash shell script is configured to delete the default VPCs and associated resources across all enabled regions on your account.  Can be particularly useful when creating a new AWS account to eliminate insecure default configurations in your environment.</p>"},{"location":"blog/2023/07/19/aws-virtual-networking-default-settings/#resources-removed","title":"Resources Removed","text":"<p>The following resource objects attached to the default VPC, in addition to the VPC itself, are removed in all enabled regions: - DHCP Options - Internet Gateway - Subnets - Security Groups - Network ACLs - Route Table - Default VPC</p>"},{"location":"blog/2023/07/19/aws-virtual-networking-default-settings/#dependencies","title":"Dependencies","text":"<ul> <li><code>awscliv2</code> installed and configured with a default profile</li> <li><code>jq</code> (for parsing JSON output from AWS CLI commands)</li> </ul>"},{"location":"blog/2023/07/19/aws-virtual-networking-default-settings/#references","title":"References","text":"<p>https://docs.aws.amazon.com/vpc/latest/userguide/default-vpc.html</p>"},{"location":"blog/2024/09/05/aws-security-assessments-with-prowler/","title":"AWS security assessments with Prowler","text":"<p>Prowler is an Open Source Security tool for AWS, Azure, GCP and Kubernetes to do security assessments, audits, incident response, compliance, continuous monitoring, hardening and forensics readiness. Includes CIS, NIST 800, NIST CSF, CISA, FedRAMP, PCI-DSS, GDPR, HIPAA, FFIEC, SOC2, GXP, Well-Architected Security, ENS and more</p>"},{"location":"blog/2024/09/05/aws-security-assessments-with-prowler/#additional-references","title":"Additional References","text":"<ol> <li> <p>https://github.com/prowler-cloud/prowler\u00a0\u21a9</p> </li> <li> <p>https://hub.docker.com/r/toniblyx/prowler/tags\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2023/07/18/azure-virtual-networking-default-settings/","title":"Azure Virtual Networking Default Settings","text":""},{"location":"blog/2023/07/18/azure-virtual-networking-default-settings/#default-vnet","title":"Default VNet","text":""},{"location":"blog/2023/07/18/azure-virtual-networking-default-settings/#default-network-security-groups-nsgs","title":"Default Network Security Groups (NSGs)","text":""},{"location":"blog/2023/07/18/azure-virtual-networking-default-settings/#deleting-the-default-vnet-and-default-subnets","title":"Deleting the Default VNet and Default Subnets","text":""},{"location":"blog/2023/07/18/azure-virtual-networking-default-settings/#references","title":"References","text":""},{"location":"blog/2024/09/02/azure-well-architected-framework/","title":"Azure Well-Architected Framework","text":"<p>The Azure Well-Architected Framework helps you to design, build, and continuously improve a secure, reliable, and efficient application. </p> <p>Start with the Pillars, and align your design choices with the principles. Then, build a strong foundation for your workload based on technical design areas. Finally, use review tools to assess your readiness in deploying to production.</p> <p>When you're building an Azure architecture, there are many considerations to keep in mind. You want your architecture to be secure, scalable, available, and recoverable. To make that possible, you have to make decisions based on cost, organizational priorities, and risk.</p>"},{"location":"blog/2024/09/02/azure-well-architected-framework/#pillars","title":"Pillars","text":"<p>The Azure Well-Architected Framework is a set of guiding tenets to build high-quality solutions on Azure. There's no one-size-fits-all approach to designing an architecture, but there are some universal concepts that apply regardless of the architecture, technology, or cloud provider.</p> <p>These concepts aren't all-inclusive, but focusing on them can help you build a reliable, secure, and flexible foundation for your application.</p> <p>The Azure Well-Architected Framework consists of five pillars:</p> Pillar Workload Concern Principles Reliability Resiliency, availability, recovery Design for business requirements, resilience, recovery, and operations, while keeping it simple. Security Data protection, threat detection, and mitigation Protect confidentiality, integrity, and availability. Cost Optimization Cost modeling, budgets, reduce waste Optimize on usage and rate utilization while keeping a cost-efficient mindset. Operational Excellence Holistic observability, DevOps practices Streamline operations with standards, comprehensive monitoring, and safe deployment practices. Performance Efficiency Scalability, load testing Scale horizontally, test early and often, and monitor the health of the solution."},{"location":"blog/2024/09/02/azure-well-architected-framework/#reliability","title":"Reliability","text":"<p>Every architect's worst fear is having an architecture fail with no way to recover it. A successful cloud environment is designed in a way that anticipates failure at all levels. Part of anticipating failures is designing a system that can recover from a failure within the time that your stakeholders and customers require.</p> <p>Design Principles</p> <ul> <li>Design for business requirements</li> <li>Design for resilience</li> <li>Design for recovery</li> <li>Design for operations</li> <li>Keep it simple</li> </ul>"},{"location":"blog/2024/09/02/azure-well-architected-framework/#security","title":"Security","text":"<p>Data is the most valuable piece of your organization's technical footprint. In this pillar, you focus on securing access to your architecture through authentication and protecting your application and data from network vulnerabilities. You should also protect the integrity of your data through tools like encryption.</p> <p>You must think about security throughout the entire lifecycle of your application, from design and implementation to deployment and operations. The cloud provides protections against various threats, such as network intrusion and DDoS attacks. But you still need to build security into your application, processes, and organizational culture.</p> <p>Design Principles</p> <ul> <li>Plan your security readiness</li> <li>Design to protect confidentiality</li> <li>Design to protect integrity</li> <li>Design to protect availability</li> <li>Sustain and evolve your security posture</li> </ul>"},{"location":"blog/2024/09/02/azure-well-architected-framework/#cost-optimization","title":"Cost Optimization","text":"<p>You want to design your cloud environment so that it's cost-effective for operations and development. Identify inefficiency and waste in cloud spending to ensure you're spending money where you can make the greatest use of it.</p> <p>Design Principles</p> <ul> <li>Develop cost-management discipline</li> <li>Design with a cost-efficiency mindset</li> <li>Design for usage optimization</li> <li>Design for rate optimization</li> <li>Monitor and optimize over time</li> </ul>"},{"location":"blog/2024/09/02/azure-well-architected-framework/#operational-excellence","title":"Operational Excellence","text":"<p>By taking advantage of modern development practices such as DevOps, you can enable faster development and deployment cycles. You need to have a good monitoring architecture in place so that you can detect failures and problems before they happen or, at a minimum, before your customers notice. Automation is a key aspect of this pillar to remove variance and error while increasing operational agility.</p> <p>Design Principles</p> <ul> <li>Embrace DevOps culture</li> <li>Establish development standards</li> <li>Evolve operations with observability</li> <li>Deploy with confidence</li> <li>Automate for efficiency</li> <li>Adopt safe deployment practices</li> </ul>"},{"location":"blog/2024/09/02/azure-well-architected-framework/#performance-efficiency","title":"Performance Efficiency","text":"<p>For an architecture to perform well and be scalable, it should properly match resource capacity to demand. Traditionally, cloud architectures accomplish this balance by scaling applications dynamically based on activity in the application. Demand for services changes, so it's important for your architecture to be able to adjust to demand. By designing your architecture with performance and scalability in mind, you provide a great experience for your customers while being cost-effective.</p> <p>Design Principles</p> <ul> <li>Negotiate realistic performance targets</li> <li>Design to meet capacity requirements</li> <li>Achieve and sustain performance</li> <li>Improve efficiency through optimization</li> </ul>"},{"location":"blog/2024/09/02/azure-well-architected-framework/#general-design-principles","title":"General Design Principles","text":"<p>In addition to each of these pillars, there are some consistent design principles that you should consider throughout your architecture.</p> <ul> <li>Enable architectural evolution: No architecture is static. Allow for the evolution of your architecture by taking advantage of new services, tools, and technologies when they're available.</li> <li>Use data to make decisions: Collect data, analyze it, and use it to make decisions surrounding your architecture. From cost data, to performance, to user load, using data can guide you to make the right choices in your environment.</li> <li>Educate and enable: Cloud technology evolves quickly. Educate your development, operations, and business teams to help them make the right decisions and build solutions to solve business problems. Document and share configurations, decisions, and best practices within your organization.</li> <li>Automate: Automation of manual activities reduces operational costs, minimizes error introduced by manual steps, and provides consistency between environments.</li> </ul> <p>In an ideal architecture, you'd build the most secure, high-performance, highly available, and efficient environment possible. However, as with everything, there are tradeoffs.</p> <p>To build an environment with the highest level of all these pillars, there's a cost. That cost might be in money, time to deliver, or operational agility. Every organization has different priorities that affect the design choices that are made in each pillar. As you design your architecture, you need to determine which trade-offs are acceptable and which aren't.</p> <p>When you're building an Azure architecture, there are many considerations to keep in mind. You want your architecture to be secure, scalable, available, and recoverable. To make that possible, you have to make decisions based on cost, organizational priorities, and risk.</p>"},{"location":"blog/2024/09/02/azure-well-architected-framework/#workloads","title":"Workloads","text":""},{"location":"blog/2024/09/02/azure-well-architected-framework/#azure-service-guides","title":"Azure Service Guides","text":""},{"location":"blog/2024/09/02/azure-well-architected-framework/#azure-architecture-center","title":"Azure Architecture Center","text":""},{"location":"blog/2024/09/02/azure-well-architected-framework/#additional-references","title":"Additional References","text":"<ul> <li>https://learn.microsoft.com/en-us/azure/well-architected/</li> <li>https://learn.microsoft.com/en-us/azure/well-architected/pillars</li> </ul>"},{"location":"blog/2023/06/01/cyber-defense-matrix/","title":"Cyber Defense Matrix","text":""},{"location":"blog/2023/06/10/threat-modeling-with-mitre-attck-navigator/","title":"Threat Modeling with MITRE ATT&amp;CK Navigator","text":""},{"location":"blog/2023/09/12/build-a-docker-container-for-cloud-cli-tools/","title":"Build a Docker Container for Cloud CLI Tools","text":"<p>My Docker container for Cloud CLI Tools.</p> <p>This Dockerfile builds on a Ubuntu image and installs CLI tools for interacting with AWS, Azure, and GCP:</p> <ul> <li><code>awscliv2</code> - AWS CLIv2</li> <li><code>azure-cli</code> - Microsoft Azure CLI</li> <li><code>gcloud</code> - Google Cloud SDK</li> </ul> <p>The image also creates and runs under a non-root user.</p>"},{"location":"blog/2023/09/12/build-a-docker-container-for-cloud-cli-tools/#using-the-image","title":"Using the Image","text":"<p>You can build the image yourself using this Dockerfile or pull from Docker Hub</p> <pre><code>docker pull mnyethecyberguy/cloud-cli\n</code></pre>"},{"location":"blog/2023/09/12/build-a-docker-container-for-cloud-cli-tools/#building-the-image-locally","title":"Building the Image Locally","text":"<p>Git clone the repository by running the following commands, one at a time:</p> <p><pre><code>git clone https://github.com/mnyethecyberguy/docker-cloud-cli.git\n</code></pre> and then</p> <p><pre><code>cd docker-cloud-cli\n</code></pre> Build the container</p> <pre><code>docker build -t cloud-cli .\n</code></pre>"},{"location":"blog/2023/09/12/build-a-docker-container-for-cloud-cli-tools/#dockerfile","title":"Dockerfile","text":"<pre><code>FROM ubuntu\n\n# Add a Non-Root user\nRUN useradd -s /bin/bash -m ubuntu &amp;&amp; \\\n    apt update &amp;&amp; \\\n    apt install -y sudo &amp;&amp; \\\n    echo \"ubuntu ALL=(ALL) NOPASSWD: ALL\" &gt;&gt; /etc/sudoers\n\nUSER ubuntu\nWORKDIR /home/ubuntu\nSHELL [\"/bin/bash\", \"-c\"]\n\n# AWS CLI\nRUN sudo apt update &amp;&amp; \\\n    sudo apt install -y curl unzip sudo &amp;&amp; \\\n    curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" &amp;&amp; \\\n    unzip awscliv2.zip &amp;&amp; \\\n    sudo ./aws/install &amp;&amp; \\\n    rm awscliv2.zip\n\n# Azure CLI\nRUN sudo apt update &amp;&amp; \\\n    sudo apt install -y ca-certificates curl apt-transport-https lsb-release gnupg &amp;&amp; \\\n    curl -sL https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/microsoft.gpg &gt; /dev/null &amp;&amp; \\\n    AZ_REPO=$(lsb_release -cs) &amp;&amp; \\\n    echo \"deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ $AZ_REPO main\" | sudo tee /etc/apt/sources.list.d/azure-cli.list &amp;&amp; \\\n    sudo apt update &amp;&amp; \\\n    sudo apt install -y azure-cli\n\n# GCloud CLI\nRUN echo deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list &amp;&amp; \\\n    curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - &amp;&amp; \\\n    sudo apt update -y &amp;&amp; \\\n    sudo apt install google-cloud-sdk -y\n</code></pre>"},{"location":"blog/2023/09/12/build-a-docker-container-for-cloud-cli-tools/#additional-references","title":"Additional References","text":"<ol> <li> <p>https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html\u00a0\u21a9</p> </li> <li> <p>https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-linux?pivots=apt\u00a0\u21a9</p> </li> <li> <p>https://cloud.google.com/sdk/docs/install\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2020/07/01/how-to-remove-images-and-containers-in-docker/","title":"How to Remove Images and Containers in Docker","text":"<p>Docker is a software platform that allows you to build, test, and deploy applications quickly. Docker packages software into standardized units called containers that have everything the software needs to run including libraries, system tools, code, and runtime. Using Docker, you can quickly deploy and scale applications into the cloud (or some other environment) and know your code will run.</p> <p>Inevitably, when working with Docker, you will need to build, destroy, and rebuild images and containers.  The commands below can help with cleaning up those resources you no longer need.</p>"},{"location":"blog/2020/07/01/how-to-remove-images-and-containers-in-docker/#removing-images","title":"Removing Images","text":""},{"location":"blog/2020/07/01/how-to-remove-images-and-containers-in-docker/#removing-an-image","title":"Removing an Image","text":"<p>To remove a single image by using the image ID or name, use the command <code>docker rmi</code>.</p> <p>To find the image name or ID, you can use <code>docker images</code> or <code>docker images -a</code>.</p> <p>Once you have the ID, you can remove the image by running <code>docker rmi &lt;IMAGE_ID&gt;</code>.</p> <p>Note</p> <p>You cannot remove an image that is being used by an existing container.  You must first delete the container and then remove the image.</p>"},{"location":"blog/2020/07/01/how-to-remove-images-and-containers-in-docker/#removing-multiple-images","title":"Removing Multiple Images","text":""},{"location":"blog/2020/07/01/how-to-remove-images-and-containers-in-docker/#removing-containers","title":"Removing Containers","text":""},{"location":"blog/2020/07/01/how-to-remove-images-and-containers-in-docker/#removing-a-container","title":"Removing a Container","text":""},{"location":"blog/2020/07/01/how-to-remove-images-and-containers-in-docker/#removing-multiple-containers","title":"Removing Multiple Containers","text":""},{"location":"blog/2023/09/01/building-an-elk-stack-with-docker-compose/","title":"Building an ELK Stack with Docker Compose","text":"<p>These files can be used to create a generic ELK stack using Docker Compose.  This can be helpful for testing, local development, and POC work.  Looking for a fast setup and teardown makes Docker perfect for this use case.</p>"},{"location":"blog/2023/09/01/building-an-elk-stack-with-docker-compose/#prerequisites","title":"Prerequisites","text":"<p>This assumes that you have <code>git</code> and <code>docker desktop</code> or <code>docker engine</code> with <code>docker-compose</code>  installed.  For this scenario, we will be using Docker Desktop.</p>"},{"location":"blog/2023/09/01/building-an-elk-stack-with-docker-compose/#file-structure","title":"File Structure","text":"<p>This is a simple file structure that will build Elasticsearch, Logstash, and Kibana from the <code>docker-compose</code> file, with additional configuration for Logstash in a yml file.  This basic structure can be expanded to add additional components such as Filebeat, Metricbeat, etc.</p> <ul> <li><code>.env</code></li> <li><code>docker-compose.yml</code></li> <li><code>logstash.conf</code></li> </ul>"},{"location":"blog/2023/09/01/building-an-elk-stack-with-docker-compose/#environment-file","title":"Environment File","text":"<p>The <code>.env</code> file is used to define variables that will be passed to docker-compose.  My sample file is below and can be customized as needed.</p> <pre><code># Password for the 'elastic' user (at least 6 characters)\nELASTIC_PASSWORD=changeme\n\n# Password for the 'kibana_system' user (at least 6 characters)\nKIBANA_PASSWORD=changeme\n\n# Version of Elastic products\nSTACK_VERSION=8.9.1\n\n# Set the cluster name\nCLUSTER_NAME=docker-cluster\n\n# Set to 'basic' or 'trial' to automatically start the 30-day trial\nLICENSE=basic\n\n# Port to expose Elasticsearch HTTP API to the host\nES_PORT=9200\n\n# Port to expose Kibana to the host\nKIBANA_PORT=5601\n\n# Increase or decrease based on the available host memory (in bytes)\nMEM_LIMIT=1073741824\n</code></pre> <p>The default passwords of \"changeme\" should always be changed, regardless of whether or not this is for a testing environment.</p> <p>The <code>STACK_VERSION</code> variable is used to reference in the <code>docker-compose.yml</code> file.  This helps ensure you are using the same version for all the components, and is also required because the Elastic Stack images do not support the <code>:latest</code> tag.</p> <p>We are also specifying port numbers and memory limits to be used.</p>"},{"location":"blog/2023/09/01/building-an-elk-stack-with-docker-compose/#containers","title":"Containers","text":"<p>An initial setup container is required as version 8.0+ of Elasticsearch has security enabled by default.  We can use this container to create the necessary certificates.</p>"},{"location":"blog/2023/09/01/building-an-elk-stack-with-docker-compose/#setup-container","title":"<code>setup</code> Container","text":"<p>The <code>setup</code> container is started first and will save the generated certificates to the <code>certs</code> volume, which makes them accessible to the other containers we are building.  It will also pass variables to the Elasticsearch container for cluster configuration.</p> <pre><code>version: \"3.8\"\n\nservices:\n  setup:\n    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}\n    volumes:\n      - certs:/usr/share/elasticsearch/config/certs\n    user: \"0\"\n    command: &gt;\n      bash -c '\n        if [ x${ELASTIC_PASSWORD} == x ]; then\n          echo \"Set the ELASTIC_PASSWORD environment variable in the .env file\";\n          exit 1;\n        elif [ x${KIBANA_PASSWORD} == x ]; then\n          echo \"Set the KIBANA_PASSWORD environment variable in the .env file\";\n          exit 1;\n        fi;\n        if [ ! -f config/certs/ca.zip ]; then\n          echo \"Creating CA\";\n          bin/elasticsearch-certutil ca --silent --pem -out config/certs/ca.zip;\n          unzip config/certs/ca.zip -d config/certs;\n        fi;\n        if [ ! -f config/certs/certs.zip ]; then\n          echo \"Creating certs\";\n          echo -ne \\\n          \"instances:\\n\"\\\n          \"  - name: es01\\n\"\\\n          \"    dns:\\n\"\\\n          \"      - es01\\n\"\\\n          \"      - localhost\\n\"\\\n          \"    ip:\\n\"\\\n          \"      - 127.0.0.1\\n\"\\\n          \"  - name: kibana\\n\"\\\n          \"    dns:\\n\"\\\n          \"      - kibana\\n\"\\\n          \"      - localhost\\n\"\\\n          \"    ip:\\n\"\\\n          \"      - 127.0.0.1\\n\"\\\n          &gt; config/certs/instances.yml;\n          bin/elasticsearch-certutil cert --silent --pem -out config/certs/certs.zip --in config/certs/instances.yml --ca-cert config/certs/ca/ca.crt --ca-key config/certs/ca/ca.key;\n          unzip config/certs/certs.zip -d config/certs;\n        fi;\n        echo \"Setting file permissions\"\n        chown -R root:root config/certs;\n        find . -type d -exec chmod 750 \\{\\} \\;;\n        find . -type f -exec chmod 640 \\{\\} \\;;\n        echo \"Waiting for Elasticsearch availability\";\n        until curl -s --cacert config/certs/ca/ca.crt https://es01:9200 | grep -q \"missing authentication credentials\"; do sleep 30; done;\n        echo \"Setting kibana_system password\";\n        until curl -s -X POST --cacert config/certs/ca/ca.crt -u \"elastic:${ELASTIC_PASSWORD}\" -H \"Content-Type: application/json\" https://es01:9200/_security/user/kibana_system/_password -d \"{\\\"password\\\":\\\"${KIBANA_PASSWORD}\\\"}\" | grep -q \"^{}\"; do sleep 10; done;\n        echo \"All done!\";\n      '\n    healthcheck:\n      test: [\"CMD-SHELL\", \"[ -f config/certs/es01/es01.crt ]\"]\n      interval: 1s\n      timeout: 5s\n      retries: 120\n</code></pre>"},{"location":"blog/2023/09/01/building-an-elk-stack-with-docker-compose/#es01-container","title":"<code>es01</code> Container","text":"<p>This container builds a single-node Elasticsearch cluster.</p> <pre><code>  es01:\n    depends_on:\n      setup:\n        condition: service_healthy\n    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}\n    volumes:\n      - certs:/usr/share/elasticsearch/config/certs\n      - esdata01:/usr/share/elasticsearch/data\n    ports:\n      - ${ES_PORT}:9200\n    environment:\n      - node.name=es01\n      - cluster.name=${CLUSTER_NAME}\n      - discovery.type=single-node\n      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}\n      - bootstrap.memory_lock=true\n      - xpack.security.enabled=true\n      - xpack.security.http.ssl.enabled=true\n      - xpack.security.http.ssl.key=certs/es01/es01.key\n      - xpack.security.http.ssl.certificate=certs/es01/es01.crt\n      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt\n      - xpack.security.transport.ssl.enabled=true\n      - xpack.security.transport.ssl.key=certs/es01/es01.key\n      - xpack.security.transport.ssl.certificate=certs/es01/es01.crt\n      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt\n      - xpack.security.transport.ssl.verification_mode=certificate\n      - xpack.license.self_generated.type=${LICENSE}\n    mem_limit: ${MEM_LIMIT}\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    healthcheck:\n      test:\n        [\n          \"CMD-SHELL\",\n          \"curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'\",\n        ]\n      interval: 10s\n      timeout: 10s\n      retries: 120\n</code></pre>"},{"location":"blog/2023/09/01/building-an-elk-stack-with-docker-compose/#kibana-container","title":"<code>kibana</code> Container","text":"<p>The Kibana container is dependent upon the Elasticsearch container running in a healthy state, and utilizes the certificates created during setup.</p> <pre><code>  kibana:\n    depends_on:\n      es01:\n        condition: service_healthy\n    image: docker.elastic.co/kibana/kibana:${STACK_VERSION}\n    volumes:\n      - certs:/usr/share/kibana/config/certs\n      - kibanadata:/usr/share/kibana/data\n    ports:\n      - ${KIBANA_PORT}:5601\n    environment:\n      - SERVERNAME=kibana\n      - ELASTICSEARCH_HOSTS=https://es01:9200\n      - ELASTICSEARCH_USERNAME=kibana_system\n      - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}\n      - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt\n    mem_limit: ${MEM_LIMIT}\n    healthcheck:\n      test:\n        [\n          \"CMD-SHELL\",\n          \"curl -s -I http://localhost:5601 | grep -q 'HTTP/1.1 302 Found'\",\n        ]\n      interval: 10s\n      timeout: 10s\n      retries: 120\n</code></pre>"},{"location":"blog/2023/09/01/building-an-elk-stack-with-docker-compose/#logstash01-container","title":"<code>logstash01</code> Container","text":"<p>Logstash is the final container to bring online, and has some additional configuration in the <code>logstash.conf</code> file.</p> <pre><code>  logstash01:\n    depends_on:\n      es01:\n        condition: service_healthy\n      kibana:\n        condition: service_healthy\n    image: docker.elastic.co/logstash/logstash:${STACK_VERSION}\n    labels:\n      co.elastic.logs/module: logstash\n    user: root\n    volumes:\n      - certs:/usr/share/logstash/certs\n      - logstashdata01:/usr/share/logstash/data\n      - \"./logstash_ingest_data/:/usr/share/logstash/ingest_data/\"\n      - \"./logstash.conf:/usr/share/logstash/pipeline/logstash.conf:ro\"\n    environment:\n      - xpack.monitoring.enabled=false\n      - ELASTIC_USER=elastic\n      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}\n      - ELASTIC_HOSTS=https://es01:9200\n</code></pre>"},{"location":"blog/2023/09/01/building-an-elk-stack-with-docker-compose/#building-the-containers-using-docker-compose","title":"Building the Containers using Docker Compose","text":"<p>Git clone the repository by running the following commands, one at a time:</p> <p><pre><code>git clone https://github.com/mnyethecyberguy/docker-elk.git\n</code></pre> and then</p> <p><pre><code>cd docker-elk\n</code></pre> Start the containers</p> <pre><code>docker compose up\n</code></pre>"},{"location":"blog/2023/09/01/building-an-elk-stack-with-docker-compose/#teardown","title":"Teardown","text":"<p>To destroy the environment, including containers and volumes, specify the <code>-v</code> option:</p> <pre><code>docker compose down -v\n</code></pre>"},{"location":"blog/2023/09/01/building-an-elk-stack-with-docker-compose/#configuration-considerations","title":"Configuration Considerations","text":""},{"location":"blog/2023/09/01/building-an-elk-stack-with-docker-compose/#additional-references","title":"Additional References","text":"<p>https://docs.docker.com/engine/reference/commandline/compose/#child-commands</p>"},{"location":"blog/2023/07/17/gcp-virtual-networking-default-settings/","title":"GCP Virtual Networking Default Settings","text":""},{"location":"blog/2023/07/17/gcp-virtual-networking-default-settings/#default-vpcs","title":"Default VPCs","text":""},{"location":"blog/2023/07/17/gcp-virtual-networking-default-settings/#default-security-groups","title":"Default Security Groups","text":""},{"location":"blog/2023/07/17/gcp-virtual-networking-default-settings/#deleting-the-default-vpc-and-default-subnets","title":"Deleting the Default VPC and Default Subnets","text":""},{"location":"blog/2023/07/17/gcp-virtual-networking-default-settings/#references","title":"References","text":""},{"location":"blog/2024/10/10/git-tutorial/","title":"Git Tutorial","text":"<p>Git<sup>1</sup> is a fast, scalable, distributed version control system. Git can seem confusing at first, but understanding a few key concepts make it much clearer. In this tutorial, we will walk through the basic git command workflow and clear up some common misconceptions.</p> <p>Before we dive into the commands, lets identify where our code is stored in <code>git</code>. The common assumption is that there are only two locations, but code doesn't only exist on GitHub or our local directory.</p> <p></p> <p>There are four main locations where our code lives in <code>git</code>:</p> <ul> <li>Local Working Directory Where we actively edit files locally</li> <li>Staging Area Temporary holding spot for changes before committing, also known as the index</li> <li>Local Repository Where we store committed changes locally</li> <li>Remote Repository A server like GitHub for sharing and backing up code</li> </ul> <p></p>","tags":["Git","Tutorials"]},{"location":"blog/2024/10/10/git-tutorial/#example-git-flow","title":"Example Git Flow","text":"<p>Most <code>git</code> commands move files between these four locations</p> <ul> <li>The first step is <code>git clone</code> to clone an existing repo so you have a local version of the project to work on complete with all its history. When you start working with a file, you are working in your local directory where you make changes to your code.</li> <li>Use <code>git add</code> when you are ready to commit changes, to stage a snapshot of those files in the staging area.</li> <li><code>git commit</code> takes a snapshot of the staging area and saves it to your local repository. This locks in those staged changes creating a permanent record that you can refer back to like a snapshot in time.</li> <li>Your code doesn't just stay on your local machine. When you are ready to share your work with your team, or back up your work, you use <code>git push</code> to send your commits to the remote repository. This is often a shared server where your team can collaborate.</li> </ul> <p></p>","tags":["Git","Tutorials"]},{"location":"blog/2024/10/10/git-tutorial/#collaboration-in-git","title":"Collaboration in Git","text":"<p>Collaboration in git is a two way exchange. To integrate your teammates work, you use <code>git pull</code>, which fetches changes from the remote repository and registers them into your local repository.</p> <p></p> <p><code>git pull</code> combines two commands:</p> <ul> <li><code>git fetch</code> which grabs the latest updates</li> <li><code>git merge</code> which integrates these updates with your work</li> </ul> <p></p> <p>There are times when you need to switch context, perhaps to fix a bug on another branch.</p> <p></p> <p>This is where <code>git checkout</code>, or <code>git switch</code> comes in. It allows you to switch between different branches to work on specific features.</p>","tags":["Git","Tutorials"]},{"location":"blog/2024/10/10/git-tutorial/#git-branching-and-merging","title":"Git Branching and Merging","text":"<p>Git branching allows you to divert from the main codebase to develop a new feature without impacting the main code.</p> <p></p> <p>Some key concepts include creating a new branch with <code>git branch</code>, switching between branches using <code>git switch</code>, merging branches together with <code>git merge</code>, or resolving merge conflicts when changes overlap.</p> <p></p> <p>Branching enables isolated development and collaboration workflows and more nuance when merging or rebasing changes from others, or managing branches.</p>","tags":["Git","Tutorials"]},{"location":"blog/2024/10/10/git-tutorial/#gui-tools","title":"GUI Tools","text":"<p>Many developers use graphical <code>git</code> tools like GitHub Desktop<sup>2</sup> and Sourcetree<sup>3</sup>. These tools provide visual interfaces and shortcuts for common commands that can help new users get started with <code>git</code> more easily.</p> <p></p>","tags":["Git","Tutorials"]},{"location":"blog/2024/10/10/git-tutorial/#additional-references","title":"Additional References","text":"<ol> <li> <p>Git for All Platforms. https://git-scm.com/ \u21a9</p> </li> <li> <p>GitHub Desktop. https://desktop.github.com/ \u21a9</p> </li> <li> <p>Sourcetree. https://www.sourcetreeapp.com/ \u21a9</p> </li> </ol>","tags":["Git","Tutorials"]},{"location":"blog/2023/08/01/deploy-a-desktop-on-aws-using-terraform/","title":"Deploy a Desktop on AWS Using Terraform","text":"<p>This terraform script deploys an Ubuntu Workstation with minimal additional software installed.  It enables SSH and RDP and uses security groups to restrict the administrative access to your current external IP address only to prevent it being wide open to the world.</p> <p>An example use case is a temporary sandbox system for surfing potentially dangerous websites.</p> <p>NOTE: Don't break the law, as AWS Terms of Service still apply and this is not exactly covert.</p>"},{"location":"blog/2023/08/01/deploy-a-desktop-on-aws-using-terraform/#prerequisites","title":"Prerequisites","text":"<ul> <li>This assumes that you have both <code>git</code> and <code>terraform</code> installed.</li> <li>You also need to have the <code>aws</code> CLI installed and configured.</li> <li>An SSH key pair should be created with the <code>.pem</code> file saved locally to reference.</li> </ul>"},{"location":"blog/2023/08/01/deploy-a-desktop-on-aws-using-terraform/#deployment","title":"Deployment","text":"<p>Git clone the repository and provision the cloud workstation by running the following commands, one at a time:</p> <p><pre><code>git clone https://github.com/mnyethecyberguy/terraform-aws-workstation.git\n</code></pre> and then</p> <pre><code>cd terraform-aws-workstation\n</code></pre> <p>Next modify the <code>terraform.tfvars</code> contents with the settings that are appropriate for your AWS Account.</p> <p>Now run these commands:</p> <pre><code>terraform init\nterraform apply\n</code></pre>"},{"location":"blog/2023/08/01/deploy-a-desktop-on-aws-using-terraform/#usage","title":"Usage","text":"<p>The cloud workstation can be accessed via either SSH or RDP. The necessary information will be an output when the script is complete.</p> <p>Sample Output:</p> <pre><code>RDP_Password = \"Changemenow\"\nRDP_UserName = \"clouduser\"\nRDP_address = \"&lt;your-EC2-public-IP&gt;\"\nssh_connection_string = \"ssh -i ~/cloud_workstation.pem ubuntu@&lt;your-EC2-public-IP&gt;\"\n</code></pre>"},{"location":"blog/2023/08/01/deploy-a-desktop-on-aws-using-terraform/#teardown","title":"Teardown","text":"<p>To destroy the workstation and all the resources created in your AWS account run the following command from the repository directory:</p> <pre><code>terraform destroy\n</code></pre>"},{"location":"blog/2023/08/01/deploy-a-desktop-on-aws-using-terraform/#additional-notes","title":"Additional Notes","text":"<p>The <code>temp-workstation.sh</code> is passed as user data to the instance and runs the first time the system boots. The script generates a log entry for almost every command as it runs and sends it to <code>/tmp/first-boot.log</code></p> <p>Since the script installs updates and some software, it may take a while. I like to watch the first boot script by watching its log when connected via SSH. Just run:</p> <pre><code>tail -f /tmp/first-boot.log\n</code></pre> <p>Otherwise, if you are connected via SSH, you will get a system message when the boot script has completed:</p> <pre><code>NOTICE: First Boot Setup Has Completed\n</code></pre> <p>After that point it is ready for an Remote Desktop (RDP) Connection. Use an RDP client, such as Microsoft Terminal Services Client (MSTSC) to RDP to the IP address that is output by the Terraform script.</p> <p>DON'T FORGET TO CHANGE THE RDP PASSWORD AFTER LOGON</p>"},{"location":"blog/2023/08/01/deploy-a-desktop-on-aws-using-terraform/#customization","title":"Customization","text":"<p>This script is configured to deploy the Ubuntu instance as a <code>t3a.xlarge</code> instance.  This will cost $0.1504 per Hour according to the EC2 On Demand Pricing Page. If you want to change this, locate the <code>resource \"aws_instance\" \"temp-workstation\"</code> block in the <code>main.tf</code> file and update the <code>instance_type</code> argument to a valid instance type. </p> <p>The default user created for RDP is named <code>clouduser</code> with a password of <code>Changemenow</code>.  You can find these settings in the <code>temp-workstation.sh</code> shell script, and an Output reference to them in the <code>main.tf</code> script.</p>"},{"location":"blog/2023/07/21/deploy-a-desktop-on-azure-using-terraform/","title":"Deploy a Desktop on Azure Using Terraform","text":"<p>This terraform script deploys an Ubuntu Workstation with minimal additional software installed.  It enables SSH and RDP and uses security groups to restrict the administrative access to your current external IP address only to prevent it being wide open to the world.</p> <p>An example use case is a temporary sandbox system for surfing potentially dangerous websites.</p> <p>NOTE: Don't break the law, as Azure Terms of Service still apply and this is not exactly covert.</p>"},{"location":"blog/2023/07/21/deploy-a-desktop-on-azure-using-terraform/#prerequisites","title":"Prerequisites","text":"<ul> <li>This assumes that you have both <code>git</code> and <code>terraform</code> installed.</li> <li>You also need to have the <code>az</code> CLI installed and configured.</li> <li>An SSH key pair should be created with the <code>.pem</code> file saved locally to reference.</li> </ul>"},{"location":"blog/2023/07/21/deploy-a-desktop-on-azure-using-terraform/#deployment","title":"Deployment","text":"<p>Git clone the repository and provision the cloud workstation by running the following commands, one at a time:</p> <p><pre><code>git clone https://github.com/mnyethecyberguy/terraform-azure-workstation.git\n</code></pre> and then</p> <pre><code>cd terraform-azure-workstation\n</code></pre> <p>Next modify the <code>terraform.tfvars</code> contents with the settings that are appropriate for your Azure Account.</p> <p>Now run these commands:</p> <pre><code>terraform init\nterraform apply\n</code></pre>"},{"location":"blog/2023/07/21/deploy-a-desktop-on-azure-using-terraform/#usage","title":"Usage","text":"<p>The cloud workstation can be accessed via either SSH or RDP. The necessary information will be an output when the script is complete.</p> <p>Sample Output:</p> <pre><code>RDP_Password = \"Changemenow\"\nRDP_UserName = \"clouduser\"\nRDP_address = \"&lt;your-AzureVM-public-IP&gt;\"\nssh_connection_string = \"ssh -i ~/cloud_workstation.pem ubuntu@&lt;your-AzureVM-public-IP&gt;\"\n</code></pre>"},{"location":"blog/2023/07/21/deploy-a-desktop-on-azure-using-terraform/#teardown","title":"Teardown","text":"<p>To destroy the workstation and all the resources created in your Azure account run the following command from the repository directory:</p> <pre><code>terraform destroy\n</code></pre>"},{"location":"blog/2023/07/21/deploy-a-desktop-on-azure-using-terraform/#additional-notes","title":"Additional Notes","text":"<p>The <code>temp-workstation.sh</code> is passed as user data to the instance and runs the first time the system boots. The script generates a log entry for almost every command as it runs and sends it to <code>/tmp/first-boot.log</code></p> <p>Since the script installs updates and some software, it may take a while. I like to watch the first boot script by watching its log when connected via SSH. Just run:</p> <pre><code>tail -f /tmp/first-boot.log\n</code></pre> <p>Otherwise, if you are connected via SSH, you will get a system message when the boot script has completed:</p> <pre><code>NOTICE: First Boot Setup Has Completed\n</code></pre> <p>After that point it is ready for an Remote Desktop (RDP) Connection. Use an RDP client, such as Microsoft Terminal Services Client (MSTSC) to RDP to the IP address that is output by the Terraform script.</p> <p>DON'T FORGET TO CHANGE THE RDP PASSWORD AFTER LOGON</p>"},{"location":"blog/2023/07/21/deploy-a-desktop-on-azure-using-terraform/#customization","title":"Customization","text":"<p>This script is configured to deploy the Ubuntu instance as a <code>Standard_DS1_v2</code> instance.  This will cost $53.29 per Month according to the Azure Linux Virtual Machines Pricing Page. If you want to change this, locate the <code>resource \"azurerm_linux_virtual_machine\" \"temp-workstation\"</code> block in the <code>main.tf</code> file and update the <code>instance_type</code> argument to a valid instance type. </p> <p>The default user created for RDP is named <code>clouduser</code> with a password of <code>Changemenow</code>.  You can find these settings in the <code>temp-workstation.sh</code> shell script, and an Output reference to them in the <code>main.tf</code> script.</p>"},{"location":"blog/2023/07/25/deploy-a-desktop-on-gcp-using-terraform/","title":"Deploy a Desktop on GCP Using Terraform","text":"<p>This terraform script deploys an Ubuntu Workstation with minimal additional software installed.  It enables SSH and RDP and uses security groups to restrict the administrative access to your current external IP address only to prevent it being wide open to the world.</p> <p>An example use case is a temporary sandbox system for surfing potentially dangerous websites.</p> <p>NOTE: Don't break the law, as GCP Terms of Service still apply and this is not exactly covert.</p>"},{"location":"blog/2023/07/25/deploy-a-desktop-on-gcp-using-terraform/#prerequisites","title":"Prerequisites","text":"<ul> <li>This assumes that you have both <code>git</code> and <code>terraform</code> installed.</li> <li>You also need to have the <code>gcloud</code> CLI installed and configured.</li> <li>An SSH key pair should be created with the <code>.pem</code> file saved locally to reference.</li> </ul>"},{"location":"blog/2023/07/25/deploy-a-desktop-on-gcp-using-terraform/#deployment","title":"Deployment","text":"<p>Git clone the repository and provision the cloud workstation by running the following commands, one at a time:</p> <p><pre><code>git clone https://github.com/mnyethecyberguy/terraform-gcp-workstation.git\n</code></pre> and then</p> <pre><code>cd terraform-gcp-workstation\n</code></pre> <p>Next modify the <code>terraform.tfvars</code> contents with the settings that are appropriate for your GCP Account.</p> <p>Now run these commands:</p> <pre><code>terraform init\nterraform apply\n</code></pre>"},{"location":"blog/2023/07/25/deploy-a-desktop-on-gcp-using-terraform/#usage","title":"Usage","text":"<p>The cloud workstation can be accessed via either SSH or RDP. The necessary information will be an output when the script is complete.</p> <p>Sample Output:</p> <pre><code>RDP_Password = \"Changemenow\"\nRDP_UserName = \"clouduser\"\nRDP_address = \"&lt;your-GCE-public-IP&gt;\"\nssh_connection_string = \"ssh -i ~/cloud_workstation.pem ubuntu@&lt;your-GCE-public-IP&gt;\"\n</code></pre>"},{"location":"blog/2023/07/25/deploy-a-desktop-on-gcp-using-terraform/#teardown","title":"Teardown","text":"<p>To destroy the workstation and all the resources created in your GCP account run the following command from the repository directory:</p> <pre><code>terraform destroy\n</code></pre>"},{"location":"blog/2023/07/25/deploy-a-desktop-on-gcp-using-terraform/#additional-notes","title":"Additional Notes","text":"<p>The <code>temp-workstation.sh</code> is passed as user data to the instance and runs the first time the system boots. The script generates a log entry for almost every command as it runs and sends it to <code>/tmp/first-boot.log</code></p> <p>Since the script installs updates and some software, it may take a while. I like to watch the first boot script by watching its log when connected via SSH. Just run:</p> <pre><code>tail -f /tmp/first-boot.log\n</code></pre> <p>Otherwise, if you are connected via SSH, you will get a system message when the boot script has completed:</p> <pre><code>NOTICE: First Boot Setup Has Completed\n</code></pre> <p>After that point it is ready for an Remote Desktop (RDP) Connection. Use an RDP client, such as Microsoft Terminal Services Client (MSTSC) to RDP to the IP address that is output by the Terraform script.</p> <p>DON'T FORGET TO CHANGE THE RDP PASSWORD AFTER LOGON</p>"},{"location":"blog/2023/07/25/deploy-a-desktop-on-gcp-using-terraform/#customization","title":"Customization","text":"<p>This script is configured to deploy the Ubuntu instance as a <code>n2-standard-2</code> instance.  This will cost $0.0366 per Hour according to the GCE VM Instance Pricing Page. If you want to change this, locate the <code>resource \"google_compute_instance\" \"temp-workstation\"</code> block in the <code>main.tf</code> file and update the <code>instance_type</code> argument to a valid instance type. </p> <p>The default user created for RDP is named <code>clouduser</code> with a password of <code>Changemenow</code>.  You can find these settings in the <code>temp-workstation.sh</code> shell script, and an Output reference to them in the <code>main.tf</code> script.</p>"},{"location":"blog/2024/10/15/from-a-to-zta-understanding-zero-trust/","title":"From A to ZTA: Understanding Zero Trust","text":"<p>Attackers have a strategy, they have a plan.  We know what they are going to be doing, we have modeled these behaviors with MITRE ATTACK and the Killchain concept.  But we know how we traditionally have started to design from the outside, trying to keep them out.  This model, Zero Trust, is all about trying to address the adversary that is already on the network, because we know that's what is happening.</p> <p>Many people are skeptical of zero trust or they will equate it to marketing. And what they are reacting to is a lot of the hype around zero trust that came because the original zero trust papers lived behind a paywall at Forrester, which is a research firm.  Many practitioners were not necessarily clients with Forrester, but every vendor was.  So every vendor would be talking about zero trust and that\u2019s how many practitioners saw zero trust was through the stain-glassed windows of some vendor marketing.</p> <p>Zero trust is a heavy topic, and I will do my best to breakthrough some of those stereotypes and misconceptions in this article.</p>","tags":["Zero Trust","Security Architecture"]},{"location":"blog/2024/10/15/from-a-to-zta-understanding-zero-trust/#zero-trust-definition","title":"Zero Trust Definition","text":"<p>With any Zero Trust discussion, we need to start by getting the concepts and terminology under our belts so that we are all speaking the same language.</p> <p>Forrester defines Zero Trust as:<sup>1</sup></p> <p>Zero Trust is an information security model that denies access to applications and data by default. Threat prevention is achieved by only granting access to networks and workloads utilizing policy informed by continuous, contextual, risk-based verification across users and their associated devices. Zero Trust advocates these three core principles: All entities are untrusted by default; least privilege access is enforced; and comprehensive security monitoring is implemented.</p> <p>This definition as a security model is critical.  Zero Trust is not a product.  Rather it is a paradigm, or a collection of concepts and ideas to minimize uncertainty in a hostile network.  No single product or vendor will make you \"Zero Trust\" regardless of what their sales team tells you.</p> <p>Zero Trust is based on the principle of \"never trust, always verify\", and aims to solve the inherent problems in placing trust in the network.  </p>","tags":["Zero Trust","Security Architecture"]},{"location":"blog/2024/10/15/from-a-to-zta-understanding-zero-trust/#core-tenets-of-zero-trust","title":"Core Tenets of Zero Trust","text":"<p>These are the tenets from the NIST special publication on Zero Trust Architecture.  These tenets mirror a lot of the key concepts from the definition above: least privilege, per-session, dynamic, strong authentication and authorization, telemetry, etc.  If you have not done so, I would strongly recommend familiarizing yourself with the concepts in this document.</p> <p></p> <p>NIST SP 800-207 Tenets of Zero Trust:<sup>2</sup></p> <ul> <li>All data sources and computing services are considered resources</li> <li>All communication is secured, regardless of network location</li> <li>Access to individual enterprise resources is granted on a per-session basis</li> <li>Access to resources is determined by dynamic policy, including the observable state of client identity, application and the requesting asset, and it may include other behavioral and environmental attributes</li> <li>The enterprise monitors and measures the integrity and security posture of all owned and associated assets</li> <li>All resource authentication and authorization is dynamic and strictly enforced before access is allowed</li> <li>The enterprise collects as much information as possible about the current state of the network infrastructure and communications and uses it to improve its security posture</li> </ul> <p>800-207 is a descriptive document, not prescriptive.  So it has definitions, tenets, and observed approaches, and details architectural components which we'll get to in a little bit.</p>","tags":["Zero Trust","Security Architecture"]},{"location":"blog/2024/10/15/from-a-to-zta-understanding-zero-trust/#zero-trust-maturity-models","title":"Zero Trust Maturity Models","text":"<p>Because Zero Trust is a strategic approach to cybersecurity, using a maturity model is a logical and effective way for organizations to assess their progress in adopting and implementing its principles. Maturity models aim to assist in the development of zero trust strategies and implementation plans, guiding resource allocation and prioritization, and promoting continuous improvement. Most maturity models for Zero Trust are broken down into 5 or 7 pillars, depending on whose model you are looking at.  </p>","tags":["Zero Trust","Security Architecture"]},{"location":"blog/2024/10/15/from-a-to-zta-understanding-zero-trust/#cisas-zero-trust-maturity-model-ztmmv2","title":"CISA's Zero Trust Maturity Model (ZTMMv2)","text":"<p>CISA's Zero Trust Maturity Model (ZTMM)<sup>3</sup>, which includes five pillars and three cross-cutting capabilities, is based on the foundations of zero trust. Within each pillar, the maturity model provides specific examples of Traditional, Initial, Advanced, and Optimal zero trust architectures.</p> <p></p> <p>The 5 pillars here are: Identity, Devices, Networks, Applications &amp; Workloads, and Data.  </p> <ul> <li>Identity: Focusing on user access management, continuous validation, and behavioral analysis.</li> <li>Devices: Inventorying, monitoring, and managing the security posture of all devices accessing the network.</li> <li>Networks: Securing communication regardless of location, with a focus on microsegmentation and encryption.</li> <li>Applications and Workloads: Securing access to and within applications and services, both on-premises and in the cloud.</li> <li>Data: Classifying, protecting, and monitoring sensitive data regardless of where it resides.</li> </ul> <p>In the CISA model, you will see that Visibility and Analytics, Automation and Orchestration, and Governance are the foundation of the pillars, or \"cross-cutting\" capabilities of the pillars.  These capabilities enhance interoperability and effectiveness.</p> <ul> <li>Visibility and Analytics: Gaining insights into network activity, user behavior, and system performance.</li> <li>Automation and Orchestration: Automating processes and orchestrating responses to security incidents.</li> <li>Governance: Establishing and enforcing policies for Zero Trust implementation and compliance. </li> </ul> <p>Some other models will have these foundational layers pulled up as pillars, but these are the core competencies of Zero Trust and are critical when trying to improve maturity of your Zero Trust Architectures.</p>","tags":["Zero Trust","Security Architecture"]},{"location":"blog/2024/10/15/from-a-to-zta-understanding-zero-trust/#zero-trust-architecture","title":"Zero Trust Architecture","text":"<p>Zero trust architecture (ZTA) is an enterprise\u2019s cybersecurity plan that utilizes zero trust concepts and encompasses component relationships, workflow planning, and access policies. ZTA enables secure authorized access to enterprise resources that are distributed across on-premises and multiple cloud environments, while enabling a hybrid workforce and partners to access resources from anywhere, at any time, from any device in support of the organization\u2019s mission.</p>","tags":["Zero Trust","Security Architecture"]},{"location":"blog/2024/10/15/from-a-to-zta-understanding-zero-trust/#general-zta-reference-architecture","title":"General ZTA Reference Architecture","text":"<p>In the NIST NCCoE publication Implementing a Zero Trust Architecture<sup>4</sup>, we see a high-level representation of the logical components of a ZTA illustrated.  It consists of three types of core components: Policy Engine (PE), Policy Administrator (PA), and Policy Enforcement Point (PEP), as well as several supporting components that assist the policy engine in making its decisions by providing data and policy rules related to areas such as identity, credential, and access management (ICAM); endpoint security; security analytics; data security; and resource protection. This diagram also shows the slicing between the control plane and data plane. </p> <p> </p>","tags":["Zero Trust","Security Architecture"]},{"location":"blog/2024/10/15/from-a-to-zta-understanding-zero-trust/#zta-core-components","title":"ZTA Core Components","text":"<code>Policy Engine (PE)</code> <p>The Policy Engine handles the ultimate decision to grant, deny, or revoke access to a resource for a given subject. The PE calculates the trust scores/confidence levels and ultimate access decisions based on enterprise policy and information from supporting components.</p> <code>Policy Administrator (PA)</code> <p>The PA executes the PE\u2019s policy decision by sending commands to the PEP to establish and terminate the communications path between the subject and the resource. It generates any session-specific authentication and authorization token or credential used by the subject to access the enterprise resource.</p> <code>Policy Enforcement Point (PEP)</code> <p>The PEP guards the trust zone that hosts one or more enterprise resources. It handles enabling, monitoring, and eventually terminating connections between subjects and enterprise resources. It operates based on commands that it receives from the PA. If an enterprise has highly distributed systems, it may have many PEPs to protect resources in different locations; it may also have multiple PEPs to support load balancing.</p> <code>Policy Decision Point (PDP)</code> <p>When combined, the functions of the PE and PA comprise a PDP. The PDP is where the decision as to whether or not to permit a subject to access a resource is made. The PIPs provide various types of telemetry and other information needed for the PDP to make informed access decisions. The PEP is the location at which this access decision is enforced.</p>","tags":["Zero Trust","Security Architecture"]},{"location":"blog/2024/10/15/from-a-to-zta-understanding-zero-trust/#zta-supporting-components","title":"ZTA Supporting Components","text":"<p>ZTA supporting components are integral to other enterprise systems and provide information that the PE uses to make to ZTA policy decisions. The sets of information that are collected by the ZTA supporting components and used as input to ZTA policy decisions are referred to as Policy Information Points (PIPs). ZTA supporting components and policy information fall into the following categories:</p> <ul> <li> <p>ICAM: ICAM components include the strategy, technology, and governance for creating, storing, and managing subject (e.g., enterprise user) accounts and identity records and their access to enterprise resources. Aspects of ICAM include:</p> <ul> <li>Identity management</li> <li>Access and credential management</li> <li>Federated identity</li> <li>Identity governance</li> <li>Multi-factor authentication</li> </ul> </li> <li> <p>Endpoint Security</p> <ul> <li> <p>Endpoint Detection and Response (EDR)/Endpoint Protection Platform (EPP)</p> <ul> <li>Host-based firewall</li> <li>Malware protection</li> <li>Vulnerability/threat mitigation</li> <li>Host intrusion protection</li> </ul> </li> <li> <p>Unified endpoint management (UEM)/mobile device management (MDM)</p> <ul> <li>Endpoint compliance</li> <li>Application protection</li> <li>Data protection enforcement</li> </ul> </li> <li> <p>Continuous diagnostics and mitigation (CDM)</p> </li> </ul> </li> <li> <p>Data Security: The data security component includes the policies that an enterprise needs to secure access to enterprise resources, as well as the means to protect data at rest and in transit. Aspects of data security include the following capabilities:</p> <ul> <li>Data discovery</li> <li>Data classification, labeling, and sanitization</li> <li>Data encryption</li> <li>Data integrity</li> <li>Data availability</li> <li>Data access protection and exfiltration</li> <li>Auditing and compliance</li> </ul> </li> <li> <p>Security Analytics</p> <ul> <li>SIEM</li> <li>SOAR</li> <li>Vulnerability scanning and assessment</li> <li>Network discovery</li> <li>Security controls validation</li> <li>Identity monitoring</li> <li>Security monitoring</li> <li>Application protection and response</li> <li>Cloud access permission manager</li> <li>Security analytics and access monitoring</li> <li>Network monitoring</li> <li>Traffic inspection</li> <li>Endpoint monitoring</li> <li>Threat intelligence</li> <li>User behavior analytics</li> <li>Firmware assurance</li> <li>Centralized management</li> </ul> </li> <li> <p>Resource protection: This category includes build components that do not fit neatly into one of the four supporting component/PIP categories enumerated above. They include components that are deployed on-premises or in the cloud to serve as proxies for a resource or otherwise protect it through monitoring and control, as well as secure desktops and workstations.</p> <ul> <li>Application connector</li> <li>Cloud workload protection</li> <li>Cloud security posture management</li> </ul> </li> </ul>","tags":["Zero Trust","Security Architecture"]},{"location":"blog/2024/10/15/from-a-to-zta-understanding-zero-trust/#zta-deployment-approaches","title":"ZTA Deployment Approaches","text":"<p>The reference architecture depicted above is intentionally general and is not meant to describe any particular ZTA deployment approach. There are several ways that an enterprise can enact a ZTA for workflows. A full ZT solution will include elements of all three approaches. The approaches include enhanced identity governance (EIG), microsegmentation, and Software Defined Perimeter (SDP).</p> <ul> <li>Enhanced Identity Governance (EIG): This approach uses the identity of actors as the key component of policy creation. Enterprise resource access policies are based on identity and assigned attributes. The primary requirement for  resource access is based on the access privileges granted to the given subject. </li> <li>Microsegmentation: This involves placing individual or groups of resources on a unique network segment protected by a gateway security component. In this approach, the enterprise places infrastructure devices such as intelligent switches (or routers) or next generation firewalls (NGFWs) or special purpose gateway devices to act as PEPs protecting each resource or small group of related resources. Alternatively (or additionally), the enterprise may choose to implement host-based micro-segmentation using software agents or firewalls on the endpoint assets.</li> <li>Software Defined Perimeter: The last approach uses the network infrastructure to implement a ZTA. This can be achieved by using an overlay network (i.e., layer 7 but also could be set up lower of the OSI network stack). These approaches frequently include concepts from Software Defined Networks (SDN). A common implementation of this includes an agent and resource gateway model that establishes a secure channel used for communication between the client and resource.</li> </ul>","tags":["Zero Trust","Security Architecture"]},{"location":"blog/2024/10/15/from-a-to-zta-understanding-zero-trust/#defining-the-zero-trust-protect-surface","title":"Defining the Zero Trust Protect Surface","text":"<p>A Protect Surface<sup>6</sup> is the area or portion of an organization\u2019s technology environment that the Zero Trust policy implementation protects. Protect Surfaces consist of Data, Applications, Assets, and Services (DAAS), that is, one or more DAAS elements. Instead of focusing on attack surface, which is an outside in view, this is an inside out view where we start with what we need to protect. Essentially shrinking the attack surface down orders of magnitude to something very small and easily known. </p> <p>Sometimes its easy to discover the data elements and assets, but then how do we actually apply the security controls in a way that makes sense to protect what is important? That is where the business information system concept comes in and we can group related elements together. Sometimes there are services that actually service several protect surfaces, e.g. DNS, DHCP, AD, NTP, that can form a protect surface on their own but, we may have an application and a database that go together and they are in their own segment, and they form a business information system.</p>","tags":["Zero Trust","Security Architecture"]},{"location":"blog/2024/10/15/from-a-to-zta-understanding-zero-trust/#elements-comprising-a-protect-surface","title":"Elements Comprising a Protect Surface","text":"<p>DAAS Elements - Data, Applications, Assets, and Services</p> <ol> <li>Data - The sensitive data that poses the greatest risk if exfiltrated or misused.</li> <li>Applications - The applications that use sensitive data or control critical assets.</li> <li>Assets - The assets of the organization, including IT, OT, and IoT devices.</li> <li>Services - The services and organization most depends on (e.g. DNS, DHCP, NTP, etc.).</li> </ol> <p>A business information system is comprised of one or more DAAS elements. Large business information systems may consist of several smaller subsystems.</p>","tags":["Zero Trust","Security Architecture"]},{"location":"blog/2024/10/15/from-a-to-zta-understanding-zero-trust/#5-step-process-to-implementation","title":"5 Step Process to Implementation","text":"<p>The 5 step process for implementation allows us to eat the elephant one bite at a time.</p> <ol> <li>Define the Protect Surface: This is an interative process and be approached one application after another. This also limits impact if something breaks to a single protect surface and supports a change management process.</li> <li>Map the Transaction and Data Flows: This is where we're asking \"how does the system work together as a system?\".</li> <li>Build a Zero Trust Architecture: Once we map the flows we need to architect a solution from the inside out. These will be custom to each protect surface as we build our zero trust environment one protect surface at a time.</li> <li>Create Zero Trust Policy: This is where we write policy. Everything in security is instantiated in policy.</li> <li>Monitor and Maintain the Environment </li> </ol> <p></p>","tags":["Zero Trust","Security Architecture"]},{"location":"blog/2024/10/15/from-a-to-zta-understanding-zero-trust/#additional-references","title":"Additional References","text":"<ol> <li> <p>\u201cThe Definition of Modern Zero Trust\u201d, David Holmes, Forrester (2022). https://www.forrester.com/blogs/the-definition-of-modern-zero-trust/ \u21a9</p> </li> <li> <p>NIST Special Publication 800-207: Zero Trust Architecture (2020). https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-207.pdf \u21a9</p> </li> <li> <p>Zero Trust Maturity Model, Version 2.0.  CISA (2023). https://www.cisa.gov/sites/default/files/2023-04/zero_trust_maturity_model_v2_508.pdf \u21a9</p> </li> <li> <p>Implementing a Zero Trust Architecture, NIST NCCoE (2020). https://www.nccoe.nist.gov/sites/default/files/legacy-files/zta-project-description-final.pdf \u21a9</p> </li> <li> <p>NIST Special Publication 1800-35: Implementing a Zero Trust Architecture (2025). https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1800-35.pdf \u21a9</p> </li> <li> <p>NSTAC Report to the President on Zero Trust and Trusted Identity Management (2022). https://www.cisa.gov/sites/default/files/publications/NSTAC%20Report%20to%20the%20President%20on%20Zero%20Trust%20and%20Trusted%20Identity%20Management.pdf \u21a9</p> </li> <li> <p>CSA Defining the Zero Trust Protect Surface (2024). https://cloudsecurityalliance.org/artifacts/defining-the-zero-trust-protect-surface \u21a9</p> </li> </ol>","tags":["Zero Trust","Security Architecture"]},{"location":"cli/awscli/","title":"AWS CLIv2 Cheat Sheet","text":""},{"location":"cli/awscli/#authentication","title":"Authentication","text":"<p>After creating an IAM user in the AWS console and creating an access key, configure the AWS CLI interactively with:</p> <pre><code>aws configure\n</code></pre>"},{"location":"cli/awscli/#show-the-signed-in-user","title":"Show the signed-in user","text":"<pre><code>aws sts get-caller-identity\n</code></pre>"},{"location":"cli/awscli/#cli-version-details","title":"CLI Version Details","text":"<pre><code>aws --version\n</code></pre>"},{"location":"cli/awscli/#filtering-and-querying","title":"Filtering and Querying","text":""},{"location":"cli/awscli/#enumerate-storage","title":"Enumerate Storage","text":"<p>Enumerate all buckets in an account:</p> <pre><code>aws s3 ls s3://\n</code></pre> <p>Enumerate all objects or blobs in a bucket:</p> <pre><code>aws s3 ls s3://&lt;BUCKET_NAME&gt;\n</code></pre>"},{"location":"cli/awscli/#uploading-and-downloading-files-from-storage","title":"Uploading and Downloading Files from Storage","text":"<p>Uploading</p> <pre><code>aws s3 cp file.txt s3://&lt;BUCKET_NAME&gt;\n</code></pre> <p>Downloading</p> <pre><code>aws s3 cp s3://&lt;BUCKET_NAME&gt;/file.txt .\n</code></pre>"},{"location":"cli/awscli/#key-management","title":"Key Management","text":"<p>Activate an access key id for an IAM user</p> <pre><code>aws iam update-access-key --access-key-id AKIA... --status Active --user-name &lt;user&gt;\n</code></pre> <p>Deactivate an access key id for an IAM user</p> <pre><code>aws iam update-access-key --access-key-id AKIA... --status Inactive --user-name &lt;user&gt;\n</code></pre> <p>Delete access key id and secret access key pair</p> <pre><code>aws iam delete-access-key --access-key-id &lt;access-key-id&gt; --user-name &lt;user&gt;\n</code></pre>"},{"location":"cli/awscli/#remote-administration","title":"Remote Administration","text":"<p>SSH to a Public Cloud VM</p> <pre><code>ssh ubuntu@$(aws ec2 describe-instances --filters Name=instance-state-name,Values=running Name=tag-value,Values=&lt;INSTANCE_NAME&gt; --query \"Reservations[0].Instances[0].PublicIpAddress\" --output text)\n</code></pre>"},{"location":"cli/awscli/#encryption-and-decryption","title":"Encryption and Decryption","text":"<p>Encrypting data</p> <pre><code>aws kms encrypt --key-id &lt;KEY_ARN_OR_ALIAS&gt; --plaintext &lt;TEXT&gt; | jq -r '.CiphertextBlob' | base64 -d &gt; encrypted.txt\n</code></pre> <p>Decrypting data</p> <pre><code>aws kms decrypt --key-id &lt;KEY_ARN_OR_ALIAS&gt; --ciphertext-blob fileb://encrypted.txt | jq -r '.Plaintext'\n</code></pre>"},{"location":"cli/azcli/","title":"Azure CLI Cheat Sheet","text":""},{"location":"cli/azcli/#authentication","title":"Authentication","text":"<p>After creating an Azure AD user in the Azure Portal and creating an access key, configure the Azure CLI interactively using browser-based authentication with:</p> <pre><code>az login\n</code></pre>"},{"location":"cli/azcli/#show-the-signed-in-user","title":"Show the signed-in user","text":"<pre><code>az ad signed-in-user show\n</code></pre> <p>Alternatively, you can ensure the CLI is configured properly by running the following command to request a new access token for the currently logged in user:</p> <pre><code>az account get-access-token\n</code></pre>"},{"location":"cli/azcli/#cli-version-details","title":"CLI Version Details","text":"<pre><code>az --version\n</code></pre>"},{"location":"cli/azcli/#filtering-and-querying","title":"Filtering and Querying","text":""},{"location":"cli/azcli/#azure-storage","title":"Azure Storage","text":""},{"location":"cli/azcli/#enumerate-storage","title":"Enumerate Storage","text":"<p>Enumerate all storage accounts in an account:</p> <pre><code>az storage account list\n</code></pre> <p>Enumerate all containers in an Azure storage account:</p> <pre><code>az storage container list --account-name &lt;STORAGE_ACCOUNT_NAME&gt;\n</code></pre> <p>Enumerate all objects or blobs in a container:</p> <pre><code>az storage blob list --account-name &lt;STORAGE_ACCOUNT_NAME&gt; --container-name &lt;CONTAINER_NAME&gt;\n</code></pre>"},{"location":"cli/azcli/#uploading-and-downloading-files-from-storage","title":"Uploading and Downloading Files from Storage","text":"<p>Uploading</p> <pre><code>az storage blob upload --account-name &lt;STORAGE_ACCOUNT_NAME&gt; --container-name &lt;CONTAINER_NAME&gt; --name file.txt --file file.txt\n</code></pre> <p>Downloading</p> <pre><code>az storage blob download --account-name &lt;STORAGE_ACCOUNT_NAME&gt; --container-name &lt;CONTAINER_NAME&gt; --name file.txt --file file.txt\n</code></pre>"},{"location":"cli/azcli/#azure-key-vaults","title":"Azure Key Vaults","text":"<p>Create a new key vault</p> <pre><code>az keyvault create --name &lt;KEY_VAULT_NAME&gt; --resource-group &lt;RESOURCE_GROUP&gt; --location &lt;REGION&gt;\n</code></pre> <p>Delete a key vault</p> <pre><code>az keyvault delete --name &lt;KEY_VAULT_NAME&gt; --resource-group &lt;RESOURCE_GROUP&gt;\n</code></pre> <p>List Azure Key Vaults for a specific resource group</p> <pre><code>az keyvault list --resource-group &lt;RESOURCE_GROUP&gt;\n</code></pre> <p>Show details for a key vault</p> <pre><code>az keyvault show --name &lt;KEY_VAULT_NAME&gt;\n</code></pre>"},{"location":"cli/azcli/#resource-groups","title":"Resource Groups","text":"<p>Create a resource group</p> <pre><code>az group create --name &lt;NAME&gt; --location &lt;REGION&gt;\n</code></pre>"},{"location":"cli/azcli/#ssh-to-a-public-cloud-vm","title":"SSH to a Public Cloud VM","text":"<pre><code>ssh ubuntu@$(az vm list-ip-addresses --query \"[?virtualMachine.name=='&lt;VM_NAME&gt;'].virtualMachine.network.publicIpAddresses[0].ipAddress\" --output tsv)\n</code></pre>"},{"location":"cli/azcli/#encrypt-and-decrypt-data","title":"Encrypt and Decrypt Data","text":""},{"location":"cli/dockercli/","title":"Docker CLI Cheat Sheet","text":""},{"location":"cli/dockercli/#images","title":"Images","text":"<p>Build an Image from a Dockerfile</p> <pre><code>docker build -t &lt;image_name&gt;\n</code></pre> <p>Build an Image from a Dockerfile without the cache</p> <pre><code>docker build -t &lt;image_name&gt; . \u2013no-cache\n</code></pre> <p>List local images</p> <pre><code>docker image ls\n</code></pre> <p>Delete an Image</p> <pre><code>docker image rm &lt;imageid&gt;\n</code></pre> <p>Remove all unused images</p> <pre><code>docker image prune\n</code></pre>"},{"location":"cli/dockercli/#containers","title":"Containers","text":"<p>Create and run a container from an image, with a custom name</p> <pre><code>docker run --name &lt;container_name&gt; &lt;image_name&gt;\n</code></pre> <p>Run a container with and publish a container\u2019s port(s) to the host</p> <pre><code>docker run -p &lt;host_port&gt;:&lt;container_port&gt; &lt;image_name&gt;\n</code></pre> <p>Run a container in the background</p> <pre><code>docker run -d &lt;image_name&gt;\n</code></pre> <p>Start an existing container</p> <pre><code>docker start &lt;container_name&gt; (or &lt;container-id&gt;)\n</code></pre> <p>Stop an existing container</p> <pre><code>docker stop &lt;container_name&gt; (or &lt;container-id&gt;)\n</code></pre> <p>Remove a stopped container</p> <pre><code>docker container rm &lt;container_name&gt;\n</code></pre> <p>Open a shell inside a running container</p> <pre><code>docker exec -it &lt;container_name&gt; sh\n</code></pre> <p>Fetch and follow the logs of a container</p> <pre><code>docker logs -f &lt;container_name&gt;\n</code></pre> <p>To inspect a running container</p> <pre><code>docker inspect &lt;container_name&gt; (or &lt;container_id&gt;)\n</code></pre> <p>To list currently running containers</p> <pre><code>docker ps\n</code></pre> <p>List all docker containers (running and stopped)</p> <pre><code>docker ps --all\n</code></pre> <p>View resource usage stats</p> <pre><code>docker container stats\n</code></pre>"},{"location":"cli/dockercli/#docker-hub","title":"Docker Hub","text":"<p>Login into Docker</p> <pre><code>docker login -u &lt;username&gt;\n</code></pre> <p>Publish an image to Docker Hub</p> <pre><code>docker push &lt;username&gt;/&lt;image_name&gt;\n</code></pre> <p>Search Hub for an image</p> <pre><code>docker search &lt;image_name&gt;\n</code></pre> <p>Pull an image from a Docker Hub</p> <pre><code>docker pull &lt;image_name&gt;:&lt;tag&gt;\n</code></pre>"},{"location":"cli/dockercli/#general-commands","title":"General Commands","text":"<p>Start the docker daemon</p> <pre><code>docker -d\n</code></pre> <p>Get help with Docker. Can also use \u2013help on all subcommands</p> <pre><code>docker --help\n</code></pre> <p>Display system-wide information</p> <pre><code>docker info\n</code></pre>"},{"location":"cli/gcloudcli/","title":"Google Cloud CLI Cheat Sheet","text":""},{"location":"cli/gcloudcli/#authentication","title":"Authentication","text":"<p>After creating a service account in the Google Cloud console and creating an access key, configure the gcloud CLI interactively with:</p> <pre><code>gcloud auth activate-service-account --key-file ~/Downloads/&lt;KEY_FILE&gt;.json\n</code></pre>"},{"location":"cli/gcloudcli/#show-the-signed-in-user","title":"Show the signed-in user","text":"<pre><code>gcloud config get-value account\n</code></pre> <p>Alternatively, you can ensure the CLI is configured properly by running the following command to request a new access token for the currently logged in user:</p> <pre><code>gcloud auth print-identity-token\n</code></pre>"},{"location":"cli/gcloudcli/#cli-version-details","title":"CLI Version Details","text":"<pre><code>gcloud --version\n</code></pre>"},{"location":"cli/gcloudcli/#filtering-and-querying","title":"Filtering and Querying","text":""},{"location":"cli/gcloudcli/#enumerate-storage","title":"Enumerate Storage","text":"<p>List all buckets in a project:</p> <pre><code>gcloud storage ls\n</code></pre> <p>Enumerate all objects or blobs in a bucket:</p> <pre><code>gcloud storage ls --recursive gs://BUCKET_NAME/**\n</code></pre>"},{"location":"cli/gcloudcli/#uploading-and-downloading-files-from-storage","title":"Uploading and Downloading Files from Storage","text":"<p>Uploading objects from files</p> <pre><code>gcloud storage cp OBJECT_LOCATION gs://DESTINATION_BUCKET_NAME\n</code></pre> <p>Downloading objects as files</p> <pre><code>gcloud storage cp gs://BUCKET_NAME/OBJECT_NAME SAVE_TO_LOCATION\n</code></pre>"},{"location":"cli/gcloudcli/#ssh-to-a-public-cloud-vm","title":"SSH to a Public Cloud VM","text":"<pre><code>to do\n</code></pre>"},{"location":"cli/gcloudcli/#encrypt-and-decrypt-data","title":"Encrypt and Decrypt Data","text":"<pre><code>to do\n</code></pre> <pre><code>to do\n</code></pre>"},{"location":"cli/gitcli/","title":"Git CLI Cheat Sheet","text":""},{"location":"cli/gitcli/#configuration","title":"Configuration","text":"<p>Configure user name and email <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your@email\"\n</code></pre></p> <p>Set automatic command line coloring <pre><code>git config --global color.ui auto\n</code></pre></p> <p>Show the global git configuration <pre><code>git config --global --list\n</code></pre></p>"},{"location":"cli/gitcli/#repository-creation-cloning","title":"Repository Creation &amp; Cloning","text":"<p>Initialize a new git repository <pre><code>git init\n</code></pre></p> <p>Clone a remote repository to your local machine <pre><code>git clone &lt;url&gt;\n</code></pre></p>"},{"location":"cli/gitcli/#basic-workflow","title":"Basic Workflow","text":"<p>Add changes to the staging area <pre><code>git add &lt;file&gt;\ngit add .\n</code></pre></p> <p>Commit changes with a message <pre><code>git commit -m \"message\"\n</code></pre></p> <p>Check the status of your repository <pre><code>git status\n</code></pre></p> <p>Show changes between working directory and staging <pre><code>git diff\n</code></pre></p> <p>Show changes staged but not yet committed <pre><code>git diff --staged\n</code></pre></p> <p>Delete file from project and stage removal for commit <pre><code>git rm &lt;file&gt;\n</code></pre></p>"},{"location":"cli/gitcli/#remote-repositories","title":"Remote Repositories","text":"<p>List all remote repositories and their URLs <pre><code>git remote -v\n</code></pre></p> <p>Add a remote repository (Use SSH URLs for remotes to enhance security) <pre><code>git remote add &lt;name&gt; &lt;url&gt;\n</code></pre></p> <p>Fetch changes from a remote repository <pre><code>git fetch &lt;remote&gt;\n</code></pre></p> <p>Fetch and merge changes from a remote repository <pre><code>git pull &lt;remote&gt; &lt;branch&gt;\n</code></pre></p> <p>Push changes to a remote repository <pre><code>git push &lt;remote&gt; &lt;branch&gt;\n</code></pre></p> <p>Reset local repository to match remote <pre><code>git reset --hard origin/&lt;branch&gt;\n</code></pre></p>"},{"location":"cli/gitcli/#branching-merging","title":"Branching &amp; Merging","text":"<p>List all branches in the repository <pre><code>git branch\n</code></pre></p> <p>Create a new branch <pre><code>git branch &lt;branch&gt;\n</code></pre></p> <p>Switch to a different branch <pre><code>git checkout &lt;branch&gt;\n</code></pre></p> <p>Create and switch to a new branch <pre><code>git checkout -b &lt;branch&gt;\n</code></pre></p> <p>Merge changes from one branch into another <pre><code>git merge &lt;branch&gt;\n</code></pre></p> <p>Rebase changes interactively <pre><code>git rebase -i &lt;branch&gt;\n</code></pre></p> <p>Squash multiple (3) commits into one <pre><code>git rebase -i HEAD~3\n</code></pre></p>"},{"location":"cli/gitcli/#undoing-changes","title":"Undoing Changes","text":"<p>Discard changes in the working directory <pre><code>git checkout -- &lt;files&gt;\n</code></pre></p> <p>Unstage changes <pre><code>git reset HEAD &lt;files&gt;\n</code></pre></p> <p>Undo the last commit (local) <pre><code>git reset --soft HEAD^\n</code></pre></p> <p>Undo the last commit and discard changes <pre><code>git reset --hard HEAD^\n</code></pre></p> <p>Change the commit message of the last commit <pre><code>git commit --amend -m \"New message\"\n</code></pre></p> <p>Revert to a previous commit <pre><code>git revert &lt;hash&gt;\n</code></pre></p> <p>Unstage files staged for commit <pre><code>git restore --staged &lt;file&gt;\n</code></pre></p>"},{"location":"cli/gitcli/#tagging","title":"Tagging","text":"<p>Create a lightweight tag <pre><code>git tag &lt;tag&gt;\n</code></pre></p> <p>Create an annotated tag <pre><code>git tag -a &lt;tag&gt; -m \"Tag message\"\n</code></pre></p> <p>Push tags to a remote repository <pre><code>git push &lt;name&gt; --tags\n</code></pre></p>"},{"location":"cli/gitcli/#stashing-changes","title":"Stashing Changes","text":"<p>Stash changes in the working directory <pre><code>git stash\n</code></pre></p> <p>Stash changes with a custom message <pre><code>git stash save \"message\"\n</code></pre></p> <p>List all stashes <pre><code>git stash list\n</code></pre></p> <p>Apply the most recent stash without removing it <pre><code>git stash apply\n</code></pre></p> <p>Apply the and remove the most recent stash <pre><code>git stash pop\n</code></pre></p>"},{"location":"cli/gitcli/#cherry-picking","title":"Cherry-Picking","text":"<p>Apply changes from a specific commit <pre><code>git cherry-pick &lt;commit&gt;\n</code></pre></p> <p>Apply changes, include a reference to original commit <pre><code>git cherry-pick -x &lt;commit&gt;\n</code></pre></p>"},{"location":"cli/gitcli/#history-logs","title":"History &amp; Logs","text":"<p>View the commit history <pre><code>git log\n</code></pre></p> <p>Show concise commit history <pre><code>git log --oneline\n</code></pre></p> <p>Show the commit history with references <pre><code>git reflog\n</code></pre></p>"},{"location":"cli/gitcli/#miscellaneous","title":"Miscellaneous","text":"<p>Show the git version <pre><code>git --version\n</code></pre></p> <p>Clean untracked files and directories <pre><code>git clean -fdX\n</code></pre></p> <p>Add all and insert them in the latest commit <pre><code>git add . &amp;&amp; git commit --amend --no-edit\n</code></pre></p>"},{"location":"cli/gitcli/#ignoring-files","title":"Ignoring Files","text":"<p>Create or edit the .gitignore file <pre><code>echo -e \"*.log\\nnode_modules/\" &gt; .gitignore\n</code></pre></p>"},{"location":"cli/jqcli/","title":"jq CLI Cheat Sheet","text":""},{"location":"cli/jqcli/#installation","title":"Installation","text":"Debian / UbuntuWindows <pre><code>sudo apt update\nsudo apt install jq\n</code></pre> <pre><code>winget install jqlang.jq\n</code></pre>"},{"location":"cli/jqcli/#general-commands","title":"General Commands","text":"<p>Output a JSON file, in pretty-print format <pre><code>jq . file.json\n</code></pre></p> <p>Pretty-print API output <pre><code>curl example.org/api/v1/users | jq .\n</code></pre></p> <p>Output all elements from arrays (or all key-value pairs from objects) in a JSON file <pre><code>jq .[] file.json\n</code></pre></p> <p>Read JSON objects from a file into an array, and output it (inverse of <code>jq .[]</code>) <pre><code>jq --slurp . file.json\n</code></pre></p> <p>Output the first element in a JSON file <pre><code>jq .[0] file.json\n</code></pre></p>"},{"location":"cli/jqcli/#mapping-transforming","title":"Mapping &amp; Transforming","text":""},{"location":"cli/jqcli/#additional-references","title":"Additional References","text":"<ol> <li> <p>https://jqlang.github.io/jq/\u00a0\u21a9</p> </li> <li> <p>https://github.com/jqlang/jq\u00a0\u21a9</p> </li> </ol>"},{"location":"cli/prowlercli/","title":"Prowler CLI Cheat Sheet","text":""},{"location":"cli/prowlercli/#general-commands","title":"General Commands","text":"<p>See what options are available for a given provider <pre><code>prowler &lt;provider&gt; --help\n</code></pre></p> <p>Show Prowler version <pre><code>prowler &lt;provider&gt; --version\n</code></pre></p>"},{"location":"cli/prowlercli/#services","title":"Services","text":"<p>List out all supported services in a provider <pre><code>prowler &lt;provider&gt; --list-services\n</code></pre></p> <p>Scan specific services only <pre><code>prowler &lt;provider&gt; --services &lt;service&gt;\n</code></pre></p> <p>Exclude specific services <pre><code>prowler &lt;provider&gt; --excluded-services &lt;service&gt;\n</code></pre></p>"},{"location":"cli/prowlercli/#checks","title":"Checks","text":"<p>List available checks for a provider <pre><code>prowler &lt;provider&gt; --list-checks\n</code></pre></p> <p>List checks by cloud provider and by service <pre><code>prowler &lt;provider&gt; --list-checks -s s3\n</code></pre></p> <p>Execute specific check(s) <pre><code>prowler &lt;provider&gt; --checks &lt;check&gt;\n</code></pre></p> <p>Exclude specific check(s) <pre><code>prowler &lt;provider&gt; --excluded-checks &lt;check&gt;\n</code></pre></p>"},{"location":"cli/prowlercli/#categories","title":"Categories","text":"<p>List the available categories in the provider <pre><code>prowler &lt;provider&gt; --list-categories\n</code></pre></p> <p>Execute specific category(s): <pre><code>prowler  &lt;provider&gt; --categories\n</code></pre></p>"},{"location":"cli/prowlercli/#miscellaneous","title":"Miscellaneous","text":"<p>Filter findings by status <pre><code>prowler &lt;provider&gt; --status [PASS, FAIL, MANUAL]\n</code></pre></p> <p>Hide the Prowler banner <pre><code>prowler &lt;provider&gt; --no-banner\n</code></pre></p>"},{"location":"cli/prowlercli/#prowler-with-aws","title":"Prowler with AWS","text":"<p>Specify an AWS region <pre><code>prowler aws -r us-west-2\n</code></pre></p> <p>Scan specific services only <pre><code>prowler aws --services s3 ec2\n</code></pre></p> <p>Execute specific check(s) <pre><code>prowler aws --checks s3_bucket_public_access\n</code></pre></p>"},{"location":"cli/prowlercli/#prowler-with-azure","title":"Prowler with Azure","text":""},{"location":"cli/prowlercli/#prowler-with-google-cloud","title":"Prowler with Google Cloud","text":""},{"location":"cli/prowlercli/#options-and-help","title":"Options and Help","text":"<pre><code>prowler aws --help\nprowler azure --help\nprowler gcp --help\n</code></pre>"},{"location":"cli/prowlercli/#additional-references","title":"Additional References","text":"<ol> <li> <p>https://github.com/prowler-cloud/prowler\u00a0\u21a9</p> </li> <li> <p>https://hub.docker.com/r/toniblyx/prowler/tags\u00a0\u21a9</p> </li> </ol>"},{"location":"resources/mkdocs-template/","title":"MkDocs Material Template","text":""},{"location":"resources/mkdocs-template/#admonition-examples","title":"Admonition Examples","text":""},{"location":"resources/mkdocs-template/#collapsible-admonition","title":"Collapsible admonition","text":"Abstract <p>This is a collapsible abstract admonition block</p>"},{"location":"resources/mkdocs-template/#collapsible-admonition-with-nested-note","title":"Collapsible admonition with nested note","text":"Expand Me! <p>This is a collapsible abstract admonition block</p> <p>Note</p> <p>This is a test nested note admonition</p> <p>Test text after the nested note.</p>"},{"location":"resources/mkdocs-template/#custom-collapsible-admonition","title":"Custom Collapsible admonition","text":"Cmd <p>This is a collapsible custom admonition block</p>"},{"location":"resources/mkdocs-template/#code-annotation-examples","title":"Code Annotation Examples","text":""},{"location":"resources/mkdocs-template/#codeblocks","title":"Codeblocks","text":"<p>Some <code>code</code> goes here.</p>"},{"location":"resources/mkdocs-template/#plain-codeblock","title":"Plain codeblock","text":"<p>A plain codeblock:</p> <pre><code>some code here\ndef myfunction()\n// some comment\n</code></pre>"},{"location":"resources/mkdocs-template/#code-for-a-specific-language","title":"Code for a specific language","text":"<p>Some more code with the <code>json</code> at the start:</p> <pre><code>{\n  \"name\": \"apple\",\n  \"color\": \"red\"\n}\n</code></pre>"},{"location":"resources/mkdocs-template/#with-a-title","title":"With a title","text":"fruit.json<pre><code>{\n  \"name\": \"apple\",\n  \"color\": \"red\"\n}\n</code></pre>"},{"location":"resources/mkdocs-template/#with-line-numbers","title":"With line numbers","text":"<pre><code>{\n  \"name\": \"apple\",\n  \"color\": \"red\"\n}\n</code></pre>"},{"location":"resources/mkdocs-template/#highlighting-lines","title":"Highlighting lines","text":"<pre><code>{\n  \"name\": \"apple\",\n  \"color\": \"red\"\n}\n</code></pre>"},{"location":"resources/mkdocs-template/#icons-and-emojis","title":"Icons and Emojis","text":""},{"location":"resources/references/","title":"References","text":""},{"location":"resources/references/#aws","title":"AWS","text":"Description URL CLI Installation https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html CIS Amazon Web Services Benchmarks https://www.cisecurity.org/benchmark/amazon_web_services/ AWS Foundational Security Best Practices https://docs.aws.amazon.com/securityhub/latest/userguide/fsbp-standard.html AWS Well-Architected Framework https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"},{"location":"resources/references/#azure","title":"Azure","text":"Description URL CLI Installation https://learn.microsoft.com/en-us/cli/azure/install-azure-cli CIS Microsoft Azure Benchmarks https://www.cisecurity.org/benchmark/azure/ Microsoft Cloud Security Benchmark https://learn.microsoft.com/en-us/security/benchmark/azure/ Azure Well-Architected Framework https://learn.microsoft.com/en-us/azure/well-architected/"},{"location":"resources/references/#gcp","title":"GCP","text":"Description URL Google Cloud Well-Architected Framework https://cloud.google.com/architecture/framework CLI Installation https://cloud.google.com/sdk/docs/install CIS Google Cloud Computing Platform Benchmarks https://www.cisecurity.org/benchmark/google_cloud_computing_platform/ Google Cloud Regions and Zones https://cloud.google.com/compute/docs/regions-zones Google Cloud Locations https://cloud.google.com/about/locations Google Cloud Roles https://cloud.google.com/iam/docs/understanding-roles/"},{"location":"resources/references/#docker","title":"Docker","text":"Description URL Docker CLI Reference https://docs.docker.com/engine/reference/commandline/cli/ Docker Compose CLI Reference https://docs.docker.com/engine/reference/commandline/compose/#child-commands"},{"location":"resources/references/#sans-cloud-security","title":"SANS Cloud Security","text":"Description URL SANS Cloud Security https://www.sans.org/cloud-security Secure Service Configuration in AWS, Azure, and GCP Poster https://www.sans.org/posters/secure-service-configuration-in-aws-azure-gcp/"},{"location":"resources/references/#mitre","title":"MITRE","text":"Description URL MITRE ATT&amp;CK Enterprise Matrix https://attack.mitre.org/matrices/enterprise/ MITRE ATT&amp;CK Cloud Matrix https://attack.mitre.org/matrices/enterprise/cloud/ MITRE ATT&amp;CK Navigator https://mitre-attack.github.io/attack-navigator/"},{"location":"resources/references/#nist","title":"NIST","text":"Description URL NIST Cybersecurity Framework (CSF) v2.0 https://www.nist.gov/cyberframework NIST SP 800-207: Zero Trust Architecture https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-207.pdf NIST NCCoE Implementing a Zero Trust Architecture https://www.nccoe.nist.gov/projects/implementing-zero-trust-architecture NIST SP 800-53 Rev. 5 https://csrc.nist.gov/pubs/sp/800/53/r5/upd1/final NIST SP 800-82 Rev. 3 https://csrc.nist.gov/pubs/sp/800/82/r3/final"},{"location":"resources/references/#cyber-forensics","title":"Cyber Forensics","text":"Description URL Autopsy Digital Forensics https://www.autopsy.com/ Rekall Memory Forensics https://github.com/google/rekall Volatility Framework (Memory Extraction) https://github.com/volatilityfoundation/volatility"},{"location":"resources/zero-trust/","title":"Zero Trust","text":""},{"location":"resources/zero-trust/#resources","title":"Resources","text":"Description URL NIST SP 800-207: Zero Trust Architecture https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-207.pdf NSTAC Report to the President on Zero Trust and Trusted Identity Management https://www.cisa.gov/sites/default/files/publications/NSTAC%20Report%20to%20the%20President%20on%20Zero%20Trust%20and%20Trusted%20Identity%20Management.pdf CSA Defining the Zero Trust Protect Surface https://cloudsecurityalliance.org/artifacts/defining-the-zero-trust-protect-surface CSA Software-Defined Perimeter (SDP) Specification v2.0 https://cloudsecurityalliance.org/artifacts/software-defined-perimeter-zero-trust-specification-v2 CSA Software-Defined Perimeter (SDP) and Zero Trust https://cloudsecurityalliance.org/artifacts/software-defined-perimeter-and-zero-trust CISA Zero Trust Maturity Model v2 https://www.cisa.gov/sites/default/files/2023-04/zero_trust_maturity_model_v2_508.pdf CISA Cloud Security Technical Reference Architecture https://www.cisa.gov/sites/default/files/2023-02/cloud_security_technical_reference_architecture_2.pdf The Definition of Modern Zero Trust, Forrester https://www.forrester.com/blogs/the-definition-of-modern-zero-trust/ Department of Defense (DoD) Zero Trust Reference Architecture v2.0 https://dodcio.defense.gov/Portals/0/Documents/Library/(U)ZT_RA_v2.0(U)_Sep22.pdf Department of Defense (DoD) Zero Trust Strategy https://dodcio.defense.gov/Portals/0/Documents/Library/DoD-ZTStrategy.pdf NIST NCCoE Implementing a Zero Trust Architecture https://www.nccoe.nist.gov/projects/implementing-zero-trust-architecture Federal Zero Trust Strategy https://www.whitehouse.gov/wp-content/uploads/2022/01/M-22-09.pdf"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2020/","title":"2020","text":""},{"location":"blog/category/zero-trust/","title":"Zero Trust","text":""},{"location":"blog/category/security-architecture/","title":"Security Architecture","text":""},{"location":"blog/category/git/","title":"Git","text":""},{"location":"blog/category/tutorials/","title":"Tutorials","text":""},{"location":"blog/category/aws/","title":"AWS","text":""},{"location":"blog/category/cloud-security/","title":"Cloud Security","text":""},{"location":"blog/category/prowler/","title":"Prowler","text":""},{"location":"blog/category/azure/","title":"Azure","text":""},{"location":"blog/category/docker/","title":"Docker","text":""},{"location":"blog/category/containers/","title":"Containers","text":""},{"location":"blog/category/gcp/","title":"GCP","text":""},{"location":"blog/category/siem/","title":"SIEM","text":""},{"location":"blog/category/terraform/","title":"Terraform","text":""},{"location":"blog/category/virtual-networks/","title":"Virtual Networks","text":""},{"location":"blog/category/threat-modeling/","title":"Threat Modeling","text":""},{"location":"blog/page/2/","title":"Blog","text":""}]}